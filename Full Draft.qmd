---
title: "Determining Heart Disease Factors"
subtitle: "Group 7"
author: "Jared Feliprada, Danny Kim, James \"Drew\" Rackovan, Kevin Pham, Tom Huynh"
date: "December 1, 2023"
date-format: long
format: pdf
editor: visual
code-block-border-left: "#dfdfdf"
---

```{r}
#| echo: false
# change working directory accordingly
setwd("D:/School/Data Science/Logistic Regression")
HeartData <- read.csv("heart.csv", stringsAsFactors=TRUE)
```

{{< pagebreak >}}

# \color{blue}{Introduction}

Cardiovascular diseases (CVDs) are the number one cause of death globally. With heart failure accounting for 4 out of 5 of these CVD deaths, it is important to identify factors that put individuals at risk. Doing so allows for early detection and giving them ample time to seek assistance. The data we utilize is taken from Kaggle and is a combination of different datasets that have not been combined before.

The source of this dataset is a collection of datasets that come from the Cleveland Clinic in Cleveland, Ohio, the Hungarian Institute of Cardiology in Budapest, Hungary, the Veterans Administration Medical Center in Long Beach, California, and the University Hospital in Zurich and Basel, Switzerland. It consists of patients that underwent angiography at the aforementioned institutions. The dataset contains 918 observations with 12 variables.

### Predictors:

1.  Age: age of the patient \[years\]

2.  Sex: sex of the patient \[M: Male, F: Female\]

3.  ChestPainType: chest pain type \[TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic\]

4.  RestingBP: resting blood pressure \[mm Hg\]

5.  Cholesterol: serum cholesterol \[mm/dl\]

6.  FastingBS: fasting blood sugar \[1: if FastingBS \> 120 mg/dl, 0: otherwise\]

7.  RestingECG: resting electrocardiogram results \[Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of \> 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria\]

8.  MaxHR: maximum heart rate achieved \[Numeric value between 60 and 202\]

9.  ExerciseAngina: exercise-induced angina \[Y: Yes, N: No\]

10. Oldpeak: oldpeak = ST \[Numeric value measured in depression\]

11. ST_Slope: the slope of the peak exercise ST segment \[Up: upsloping, Flat: flat, Down: downsloping\]

### Response:

12. HeartDisease: output class \[1: heart disease, 0: Normal\]

## Main Question

Our priority is inference. The main question we are studying is which factor(s) contribute the most to the risk of heart disease?

# \color{blue}{Methods and Results}

## Logistic Regression (Danny Kim, Jared Feliprada, James "Drew" Rackovan)

Since our dataset utilizes a mix of qualitative and quantitative variables and our response variable is a binary value, we decided to utilize a logistic regression model, as logistic models are designed for binary outcomes and its ability to handle both types of predictors. Had linear regression been used, it would predict values outside of the intended range of \[0,1\].

An advantage of the logistic regression model is its interpretability. This works well for us since our goal is to look at the relationship between the predictors and the response variable. On the other hand, the model has the disadvantage of not being the best fit if the relationship between the variables turn out to be very complex.

To begin the process of model selection, we first fit the model using all the predictors.

```{r}
#| echo: false
#| comment: "##"

# fit logistic regression model using all predictors
glm.heart <- glm(HeartDisease~., "binomial", HeartData)
summary(glm.heart)
```

From the significance table, we noted that the predictors Age, RestingBP, RestingECG, and MaxHR had p-values \> 0.05. We next performed backward stepwise regression to see its results.

```{r}
#| echo: false
#| comment: "##"

# perform backward stepwise regression
sglm.heart <- step(glm.heart, trace = 0)
summary(sglm.heart)
```

We can see from the results of the backward stepwise regression that the same high p-value predictors were removed, except for Age. With Age having a p-value just over 0.05 of 0.051770, we decided to keep it in and use the results of the backward stepwise regression for our final model.

The formula for our final model is:

$$
p(HeartDisease = 1 | X) =
$$

$$
\frac{e^{\beta_0+\beta_1(Age)+\beta_2(Sex)+\beta_3(ChestPainType)+\beta_4(Cholesterol)+\beta_5(FastingBS)+\beta_6(ExerciseAngina)+\beta_7(Oldpeak)+\beta_8(ST\_Slope)}}{1+e^{\beta_0+\beta_1(Age)+\beta_2(Sex)+\beta_3(ChestPainType)+\beta_4(Cholesterol)+\beta_5(FastingBS)+\beta_6(ExerciseAngina)+\beta_7(Oldpeak)+\beta_8(ST\_Slope)}}
$$

Using the final model, we performed 10 random 80/20 training/test splits to get the mean test prediction error.

```{r}
#| comment: "##"

# calculate mean test prediction error
set.seed(1)
test.errors <- c()
for (i in 1:10) {
  # random 80/20 training/test split
  sample <- sample(nrow(HeartData), 0.8*nrow(HeartData))
  train.HeartData <- HeartData[sample, ]
  test.HeartData <- HeartData[-sample, ]
  
  # fit model using results of backward stepwise regression
  train.glm.heart <- glm(formula(sglm.heart), "binomial", train.HeartData)
  
  # calculate test prediction error
  glm.pred = predict(train.glm.heart, test.HeartData, "response")
  glm.pred = ifelse(glm.pred >= 0.5, 1, 0)
  test.errors[i] <- sum(glm.pred != test.HeartData[, "HeartDisease"])/length(glm.pred)
}
cat("Mean test prediction error: ", mean(test.errors))
```

```{r}
#| echo: false
#| comment: "##"

# repeat significance table of final model
summary(sglm.heart)
```

From the results of our final model fit on the full dataset, we can see the most significant predictors are Sex, ChestPainType, Cholesterol, FastingBS, and ExerciseAngina, which have the smallest p-values among all the predictors; ST_SlopeFlat is also a significant predictor, but has a larger p value in comparison. Among these, ChestPainTypeNAP appears to be the most statistically significant predictor, as it has the smallest p-value overall. Oldpeak has some significance but not to the same extent as all the other variables.

## Decision Tree (Kevin Pham, Tom Huynh)

```{r}
#| echo: false
library(tree)
heart$HeartDisease=as.factor(heart$HeartDisease)
heart$ChestPainType=as.factor(heart$ChestPainType)
heart$RestingECG=as.factor(heart$RestingECG)
heart$ExerciseAngina=as.factor(heart$ExerciseAngina)
heart$ST_Slope=as.factor(heart$ST_Slope)
```

Model Formula = HeartDisease ~ Age+ Sex+ ChestPainType+ RestingBP + Cholesterol + FastingBS + RestingECG + MaxHR + ExerciseAngina + Oldpeak + ST_Slope

Using a decision tree in this situation allows us to interpret the data much more easily compared to other models and grants us the ability to recognize non-linear relationships, outliers, multi-factor interactions, and more. However, there are several drawbacks commonly associated with decision trees such as their high variance and risk of overfitting. With the Heart dataset, there are a total of 10 nodes in which we can utilize pruning to select the best nodes/factors to obtain the best accuracy.

```{r}
#| echo: true
#| warning: false
set.seed(100)
tree.HD = tree(HeartDisease~., data = heart)
plot(tree.HD)
text(tree.HD, pretty=0)
```

In order to find the best number of nodes for our decision tree, we can use cross-validation to do so in a way that minimizes misclassification error. Cross-validation is typically performed by iteratively splitting the data into training and validation sets. For each split, the model is trained on the training set and evaluated on the validation set.

The cv.tree function performs cross-validation on the decision tree. It takes the initial tree (tree.HD) and a pruning function (prune.misclass) as arguments. Based on the plot produced, we can conclude that 7 nodes are ideal as this is when we see the plots start to level.

```{r}
#| echo: true
#| warning: false
set.seed(100)
cv.HD = cv.tree(tree.HD,FUN=prune.misclass)
plot(cv.HD$size,cv.HD$dev,type="b")
```

After constructing the initial decision tree, pruning involves removing branches to simplify the model. This helps prevent overfitting, where the tree captures noise in the training data and doesn't generalize well to new data. The plot of the pruned tree shows a simplified structure compared to the original tree. Pruning helps in achieving a balance between model complexity and predictive performance. The pruned tree cuts the total number of nodes down to 7 from 10.

```{r}
#| echo: true
prune.HD=prune.misclass(tree.HD,best=7)
plot(prune.HD)
text(prune.HD, pretty=2)
```

The dataset is split into 80/20 training and testing sets to get the mean test error rate. The entire process is repeated 10 times with different random seeds. This repetition accounts for the variability in training/test set splits and provides a more robust assessment of the model's performance.

This average error rate serves as an overall indicator of the model's predictive performance. A lower mean error rate indicates better accuracy in predicting the target variable.

```{r}
#| echo: true
#| warning: false
error=rep(0,10)
for(i in 1:10){
  set.seed(i)
  trainingIndex = sample(1:nrow(heart),.8*nrow(heart))
  train = heart[trainingIndex,]
  test = heart[-trainingIndex,]
  
  tree.HD = tree(HeartDisease~., data = train)
  prune.HD = prune.misclass(tree.HD,best=7)
  if(i == 1 || i == 4){
    plot(prune.HD)
    text(prune.HD,pretty=2)
  }
  
  yhat=predict(prune.HD,newdata=test,type="class")
  error[i] = sum(yhat != test$HeartDisease) / length(yhat)
}
mean(error)
error
```

The resulting two trees were picked based on the lowest error rate across all 10 trees, with these error rates being .136 and .125 respectively. From these two trees, we see that when combined with one another, the most significant combinations of factors are ST_Slope, ChestPainType, MaxHR, Oldpeak, and Cholesterol. For both tree models, ST_Slope is the predictor used for the main split. Going down a level, the model chooses ChestPainType for both, and then Oldpeak, MaxHR, and Cholesterol are used to determine the final nodes.


{{< pagebreak >}}

# \color{blue}{Conclusion}

Overall, both models determined that ChestPainType, Cholesterol, and ST_Slope are the most significant predictors in determining a patient's risk for heart disease. However, there are slight discrepancies between the models. The logistic regression model identified FastingBS, ExerciseAngina, and Sex as significant predictors, while the tree model omitted these variables from the pruned tree.

Some possible reasons for the discrepancies include the linear nature of the logistic regression model compared to the non-linear nature of the tree model. The tree model can capture more complex interactions between variables, in addition to both models having different variable importance measures. The tree model is also more flexible as it makes fewer assumptions about the data.

# Bibliography

Janosi, A, Steinbrunn, W, Pfisterer, M, and Detrano, R. (1988). Heart Failure Prediction Dataset. Retrieved December 1, 2023 from https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data.
