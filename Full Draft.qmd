---
title: "Determining Heart Disease Factors"
subtitle: "Group 7"
author: "Jared Feliprada, Danny Kim, James \"Drew\" Rackovan, Kevin Pham, Tom Huynh"
date: "December 1, 2023"
date-format: long
format: pdf
editor: visual
code-block-border-left: "#dfdfdf"
---

```{r}
#| echo: false
# change working directory accordingly
setwd("D:/School/Data Science/Logistic Regression")
HeartData <- read.csv("heart.csv", stringsAsFactors=TRUE)
```

{{< pagebreak >}}

# \color{blue}{Introduction}

Cardiovascular diseases (CVDs) are the number one cause of death globally. With heart failure accounting for 4 out of 5 of these CVD deaths, it is important to identify factors that put individuals at risk. Doing so allows for early detection and giving them ample time to seek assistance. The data we utilize is taken from Kaggle and is a combination of different datasets that have not been combined before.

The source of this dataset is a collection of datasets that come from the Cleveland Clinic in Cleveland, Ohio, the Hungarian Institute of Cardiology in Budapest, Hungary, the Veterans Administration Medical Center in Long Beach, California, and the University Hospital in Zurich and Basel, Switzerland. It consists of patients that underwent angiography at the aforementioned institutions. The dataset contains 918 observations with 12 variables.

### Predictors:

1.  Age: age of the patient \[years\]

2.  Sex: sex of the patient \[M: Male, F: Female\]

3.  ChestPainType: chest pain type \[TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic\]

4.  RestingBP: resting blood pressure \[mm Hg\]

5.  Cholesterol: serum cholesterol \[mm/dl\]

6.  FastingBS: fasting blood sugar \[1: if FastingBS \> 120 mg/dl, 0: otherwise\]

7.  RestingECG: resting electrocardiogram results \[Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of \> 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria\]

8.  MaxHR: maximum heart rate achieved \[Numeric value between 60 and 202\]

9.  ExerciseAngina: exercise-induced angina \[Y: Yes, N: No\]

10. Oldpeak: oldpeak = ST \[Numeric value measured in depression\]

11. ST_Slope: the slope of the peak exercise ST segment \[Up: upsloping, Flat: flat, Down: downsloping\]

### Response:

12. HeartDisease: output class \[1: heart disease, 0: Normal\]

## Main Question

Our priority is inference. The main question we are studying is which factor(s) contribute the most to the risk of heart disease?

# \color{blue}{Methods and Results}

## Logistic Regression (Danny Kim, Jared Feliprada, James "Drew" Rackovan)

Since our dataset utilizes a mix of qualitative and quantitative variables and our response variable is a binary value, we decided to utilize a logistic regression model, as logistic models are designed for binary outcomes and its ability to handle both types of predictors. Had linear regression been used, it would predict values outside of the intended range of \[0,1\].

An advantage of the logistic regression model is its interpretability. This works well for us since our goal is to look at the relationship between the predictors and the response variable. On the other hand, the model has the disadvantage of not being the best fit if the relationship between the variables turn out to be very complex.

To begin the process of model selection, we first fit the model using all the predictors.

```{r}
#| echo: false
#| comment: "##"

# fit logistic regression model using all predictors
glm.heart <- glm(HeartDisease~., "binomial", HeartData)
summary(glm.heart)
```

From the significance table, we noted that the predictors Age, RestingBP, RestingECG, and MaxHR had p-values \> 0.05. We next performed backward stepwise regression to see its results.

```{r}
#| echo: false
#| comment: "##"

# perform backward stepwise regression
sglm.heart <- step(glm.heart, trace = 0)
summary(sglm.heart)
```

We can see from the results of the backward stepwise regression that the same high p-value predictors were removed, except for Age. With Age having a p-value just over 0.05 of 0.051770, we decided to keep it in and use the results of the backward stepwise regression for our final model.

The formula for our final model is:

$$
p(HeartDisease = 1 | X) =
$$

$$
\frac{e^{\beta_0+\beta_1(Age)+\beta_2(Sex)+\beta_3(ChestPainType)+\beta_4(Cholesterol)+\beta_5(FastingBS)+\beta_6(ExerciseAngina)+\beta_7(Oldpeak)+\beta_8(ST\_Slope)}}{1+e^{\beta_0+\beta_1(Age)+\beta_2(Sex)+\beta_3(ChestPainType)+\beta_4(Cholesterol)+\beta_5(FastingBS)+\beta_6(ExerciseAngina)+\beta_7(Oldpeak)+\beta_8(ST\_Slope)}}
$$

Using the final model, we performed 10 random 80/20 training/test splits to get the mean test prediction error.

```{r}
#| comment: "##"

# calculate mean test prediction error
set.seed(1)
test.errors <- c()
for (i in 1:10) {
  # random 80/20 training/test split
  sample <- sample(nrow(HeartData), 0.8*nrow(HeartData))
  train.HeartData <- HeartData[sample, ]
  test.HeartData <- HeartData[-sample, ]
  
  # fit model using results of backward stepwise regression
  train.glm.heart <- glm(formula(sglm.heart), "binomial", train.HeartData)
  
  # calculate test prediction error
  glm.pred = predict(train.glm.heart, test.HeartData, "response")
  glm.pred = ifelse(glm.pred >= 0.5, 1, 0)
  test.errors[i] <- sum(glm.pred != test.HeartData[, "HeartDisease"])/length(glm.pred)
}
cat("Mean test prediction error: ", mean(test.errors))
```

```{r}
#| echo: false
#| comment: "##"

# repeat significance table of final model
summary(sglm.heart)
```

From the results of our final model fit on the full dataset, we can see the most significant predictors are Sex, ChestPainType, Cholesterol, FastingBS, and ExerciseAngina, which have the smallest p-values among all the predictors; ST_SlopeFlat is also a significant predictor, but has a larger p value in comparison. Among these, ChestPainTypeNAP appears to be the most statistically significant predictor, as it has the smallest p-value overall. Oldpeak has some significance but not to the same extent as all the other variables.

## Decision Tree (Kevin Pham, Tom Huynh)

```{r}
#| echo: false
library(tree)
HeartData$HeartDisease=as.factor(HeartData$HeartDisease)
HeartData$ChestPainType=as.factor(HeartData$ChestPainType)
HeartData$RestingECG=as.factor(HeartData$RestingECG)
HeartData$ExerciseAngina=as.factor(HeartData$ExerciseAngina)
HeartData$ST_Slope=as.factor(HeartData$ST_Slope)
```

Initial Decision Tree (talk about issues, too many nodes etc)

```{r}
#| echo: true
#| warning: false
set.seed(100)
tree.HD = tree(HeartDisease~., data = HeartData)
plot(tree.HD)
text(tree.HD, pretty=0)
```

Find best number of nodes

```{r}
#| echo: true
#| warning: false
set.seed(100)
cv.HD = cv.tree(tree.HD,FUN=prune.misclass)
plot(cv.HD$size,cv.HD$dev,type="b")
```

Pruning tree

```{r}
#| echo: true
prune.HD=prune.misclass(tree.HD,best=7)
plot(prune.HD)
text(prune.HD, pretty=2)
```

Testing/training data and showing MSE

```{r}
#| echo: true
#| warning: false
MSE=rep(0,10)
for(i in 1:10){
  set.seed(i)
  trainingIndex = sample(1:nrow(HeartData),.8*nrow(HeartData))
  train = HeartData[trainingIndex,]
  test = HeartData[-trainingIndex,]
  prune.HD = prune.misclass(tree.HD,best=7)
  yhat=predict(prune.HD,newdata=test,type="class")
  MSE[i] = mean((as.numeric(yhat) - as.numeric(test$HeartDisease))^2)
}
mean(MSE)
```

From the results shown here, the most significant variables are ST_Slope, ChestPainType, Oldpeak, Cholesterol, and MaxHR.

{{< pagebreak >}}

# \color{blue}{Conclusion}

Overall, both models determined that ChestPainType, Cholesterol, and ST_Slope are the most significant predictors in determining a patient's risk for heart disease. However, there are slight discrepancies between the models. The logistic regression model identified FastingBS, ExerciseAngina, and Sex as significant predictors, while the tree model omitted these variables from the pruned tree.

Some possible reasons for the discrepancies include the linear nature of the logistic regression model compared to the non-linear nature of the tree model. The tree model can capture more complex interactions between variables, in addition to both models having different variable importance measures. The tree model is also more flexible as it makes fewer assumptions about the data.

# Bibliography

Janosi, A, Steinbrunn, W, Pfisterer, M, and Detrano, R. (1988). Heart Failure Prediction Dataset. Retrieved December 1, 2023 from https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data.
