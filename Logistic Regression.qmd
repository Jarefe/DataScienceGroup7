---
title: "Determining Heart Disease Factors"
subtitle: "Group 7"
author: "Jared Feliprada, Danny Kim, James \"Drew\" Rackovan, Kevin Pham, Tom Huynh"
date: "December 1, 2023"
date-format: long
format: pdf
editor: visual
code-block-border-left: "#dfdfdf"
---

```{r}
#| echo: false

# change working directory accordingly
setwd("D:/School/Data Science/Logistic Regression")
HeartData <- read.csv("heart.csv", stringsAsFactors=TRUE)
```

{{< pagebreak >}}

# \color{blue}{Introduction}

Cardiovascular diseases (CVDs) are the number one cause of death globally. With heart failure accounting for 4 out of 5 of these CVD deaths, it is important to identify factors that put individuals at risk. Doing so allows for early detection and giving them ample time to seek assistance. The data we utilize is taken from Kaggle and is a combination of different datasets that have not been combined before.

The source of this dataset is a collection of datasets that come from the Cleveland Clinic in Cleveland, Ohio, the Hungarian Institute of Cardiology in Budapest, Hungary, the Veterans Administration Medical Center in Long Beach, California, and the University Hospital in Zurich and Basel, Switzerland. It consists of patients that underwent angiography at the aforementioned institutions. The dataset contains 918 observations with 12 variables.

### Predictors:

1.  Age: age of the patient \[years\]

2.  Sex: sex of the patient \[M: Male, F: Female\]

3.  ChestPainType: chest pain type \[TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic\]

4.  RestingBP: resting blood pressure \[mm Hg\]

5.  Cholesterol: serum cholesterol \[mm/dl\]

6.  FastingBS: fasting blood sugar \[1: if FastingBS \> 120 mg/dl, 0: otherwise\]

7.  RestingECG: resting electrocardiogram results \[Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of \> 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria\]

8.  MaxHR: maximum heart rate achieved \[Numeric value between 60 and 202\]

9.  ExerciseAngina: exercise-induced angina \[Y: Yes, N: No\]

10. Oldpeak: oldpeak = ST \[Numeric value measured in depression\]

11. ST_Slope: the slope of the peak exercise ST segment \[Up: upsloping, Flat: flat, Down: downsloping\]

### Response:

12. HeartDisease: output class \[1: heart disease, 0: Normal\]

## Main Question

Our priority is inference. The main question we are studying is which factor(s) contribute the most to the risk of heart disease?

# \color{blue}{Methods and Results}

## Logistic Regression (Danny Kim, Jared Feliprada, James "Drew" Rackovan)

Since our dataset utilizes a mix of qualitative and quantitative variables and our response variable is a binary value, we decided to utilize a logistic regression model, as logistic models are designed for binary outcomes and its ability to handle both types of predictors. Had linear regression been used, it would predict values outside of the intended range of \[0,1\].

An advantage of the logistic regression model is its interpretability. This works well for us since our goal is to look at the relationship between the predictors and the response variable. On the other hand, the model has the disadvantage of not being the best fit if the relationship between the variables turn out to be very complex.

To begin the process of model selection, we first fit the model using all the predictors.

```{r}
#| echo: false
#| comment: "##"

# fit logistic regression model using all predictors
glm.heart <- glm(HeartDisease~., "binomial", HeartData)
summary(glm.heart)
```

From the significance table, we noted that the predictors Age, RestingBP, RestingECG, and MaxHR had p-values \> 0.05. We next performed backward stepwise regression to see its results.

```{r}
#| echo: false
#| comment: "##"

# perform backward stepwise regression
sglm.heart <- step(glm.heart, trace = 0)
summary(sglm.heart)
```

We can see from the results of the backward stepwise regression that the same high p-value predictors were removed, except for Age. With Age having a p-value just over 0.05 of 0.051770, we decided to keep it in and use the results of the backward stepwise regression for our final model.

The formula for our final model is:

$$
p(HeartDisease = 1 | X) =
$$

$$
\frac{e^{\beta_0+\beta_1(Age)+\beta_2(Sex)+\beta_3(ChestPainType)+\beta_4(Cholesterol)+\beta_5(FastingBS)+\beta_6(ExerciseAngina)+\beta_7(Oldpeak)+\beta_8(ST\_Slope)}}{1+e^{\beta_0+\beta_1(Age)+\beta_2(Sex)+\beta_3(ChestPainType)+\beta_4(Cholesterol)+\beta_5(FastingBS)+\beta_6(ExerciseAngina)+\beta_7(Oldpeak)+\beta_8(ST\_Slope)}}
$$

Using the final model, we performed 10 random 80/20 training/test splits to get the mean test prediction error.

```{r}
#| comment: "##"

# calculate mean test prediction error
set.seed(1)
test.errors <- c()
for (i in 1:10) {
  # random 80/20 training/test split
  sample <- sample(nrow(HeartData), 0.8*nrow(HeartData))
  train.HeartData <- HeartData[sample, ]
  test.HeartData <- HeartData[-sample, ]
  
  # fit model using results of backward stepwise regression
  train.glm.heart <- glm(formula(sglm.heart), "binomial", train.HeartData)
  
  # calculate test prediction error
  glm.pred = predict(train.glm.heart, test.HeartData, "response")
  glm.pred = ifelse(glm.pred >= 0.5, 1, 0)
  test.errors[i] <- sum(glm.pred != test.HeartData[, "HeartDisease"])/length(glm.pred)
}
cat("Mean test prediction error: ", mean(test.errors))
```

The following section is the same process, but with a different approach to the splitting and training

```{r}
#| echo: false
#| output: false

library(boot)
library(caret)
```

```{r}
#| comment: "##"

# calculate mean test prediction error
set.seed(1)
test.errors <- c()

# use createDataPartition for train/test split
splitIndex <- createDataPartition(HeartData$HeartDisease, p = 0.8, list = FALSE)
train.HeartData <- HeartData[splitIndex, ]
test.HeartData <- HeartData[-splitIndex, ]

# fit model using results of backward stepwise regression
train.glm.heart <- glm(HeartDisease ~ ., family = "binomial", data = train.HeartData)

# predict on test data
glm.pred <- predict(train.glm.heart, test.HeartData, type = "response")

# simplify thresholding
glm.pred <- as.numeric(glm.pred >= 0.5)

# calculate test prediction error using cross-validation
cv.error <- cv.glm(HeartData, glm.heart, K = 10)$delta[1]
cat("Mean test prediction error: ", cv.error)
```

This approach results in a smaller mean test prediction error than the previously utilized for loop, but still has the same outcome when determining the most statistically significant predictors.

```{r}
#| echo: false
#| comment: "##"

# repeat significance table of final model
summary(sglm.heart)
```

From the results of our final model fit on the full dataset, we can see the most significant predictors are Sex, ChestPainType, Cholesterol, FastingBS, and ExerciseAngina, which have the smallest p-values among all the predictors. Among these, ChestPainTypeNAP appears to be the most statistically significant predictor, as it has the smallest p-value overall.

# \color{blue}{Conclusion}

{{< pagebreak >}} 

# Bibliography

Janosi, A, Steinbrunn, W, Pfisterer, M, and Detrano, R. (1988). Heart Failure Prediction Dataset. Retrieved December 1, 2023 from https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data.
